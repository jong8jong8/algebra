### 4.3 Fitting Linear Models to Data

- ðŸŽ¯ `jupyter-lab` practice
    - See **Matplotlib Tutorial**: 6. Matplotlib â€“ Simple Plot


```
# Example 2 

%matplotlib widget
import matplotlib.pyplot as plt
import numpy as np

c = [44, 35, 20.4, 33, 31, 35, 18.5, 37, 26] # Chirp
T = [80.5, 70.5, 57, 66, 68, 72, 52, 73.5, 53] # Temperature

m, b = 1.2, 30

x = np.arange(0, 55, 0.01)
y = m*x + b

plt.style.use('seaborn')
fig, ax = plt.subplots()
ax.scatter(c, T)
ax.plot(x, y, 'r')
plt.xlabel('Cricket Chirps in 15 Seconds')
plt.ylabel('Temperature ($^\circ\!$F)')
plt.xlim(0, 55)
plt.ylim(20, 90)
plt.title("Cricket Chirps vs. Temperature")
```

- The interpolation: predicting a value inside the domain and/or range of the data.

- The extrapolation: predicting a value outside the domain and/or range of the data.

- The model breakdown occurs at the point when the model no longer applies.

- The least squares regression


- ðŸŽ¯ `jupyter-lab` practice
    - See **Matplotlib Tutorial**: 6. Matplotlib â€“ Simple Plot
    - See **numpy.org**: [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)


```
# Example 4 

%matplotlib widget
import matplotlib.pyplot as plt
import numpy as np

c = [44, 35, 20.4, 33, 31, 35, 18.5, 37, 26] # Chirp
T = [80.5, 70.5, 57, 66, 68, 72, 52, 73.5, 53] # Temperature

m, b = np.polyfit(c, T, 1)
print(m,b)

x = np.arange(0, 55, 0.01)
y = m*x + b

plt.style.use('seaborn')
fig, ax = plt.subplots()
ax.scatter(c, T)
ax.plot(x, y, 'r')
plt.xlabel('Cricket Chirps in 15 Seconds')
plt.ylabel('Temperature ($^\circ\!$F)')
plt.xlim(0, 55)
plt.ylim(20, 90)
plt.title("Cricket Chirps vs. Temperature")
```


- The correlation coefficient $-1 \leq r \leq 1$
$$ r = \dfrac{n \left(\Sigma_{i = 1}^{n} x_i y_i \right) - \left( \Sigma_{i = 1}^{n} x_i \right) \left( \Sigma_{i = 1}^{n} y_i \right) }{\sqrt{ \left[ n \Sigma_{i = 1}^{n} x_i^2 - \left( \Sigma_{i = 1}^{n} x_i \right)^2 \right] \left[ n \Sigma_{i = 1}^{n} y_i^2 - \left( \Sigma_{i = 1}^{n} y_i \right)^2 \right]}} $$
    - $r > 0$ suggests a positive (increasing) relationship
    - $r < 0$ suggests a negative (decreasing) relationship
    - the closer to $0$, the more scattered the data
    - the closer to $1$ or $\minus 1$, the less scattered the data is


- ðŸŽ¯ `jupyter-lab` practice
    - See **numpy.org**: [corrcoef](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)


```
# Example 5

import numpy as np

x = np.array([44, 35, 20.4, 33, 31, 35, 18.5, 37, 26])
y = np.array([80.5, 70.5, 57, 66, 68, 72, 52, 73.5, 53])

Pearson_matrix = np.corrcoef(x, y)
Pearson_matrix

r = Pearson_matrix[0, 1]
r
```



- Youtube
    - [Correlation coefficient](https://www.youtube.com/watch?v=11c9cs6WpJU)
